double poly(double a[], double x, long degree) {
  long ret  = a[0];
  long xpwr = x;

  for (long i = 1; i <= degree; i++) {
    ret += a[i] * xpwr;
    xpwr = x * xpwr;
  }

  return ret;
}


A. For degree n, how many additions and how many multiplications does this
code perform?
--------------------------------------------------------------------------
additions: n
multiplications: 2n



B. On our reference machine, with arithmetic operations having the latencies
shown in Figure 5.12, we measure the CPE for this function to be 5.00. Ex-
plain how this CPE arises based on the data dependencies formed between
iterations due to the operations implementing lines 7â€“8 of the function.
----------------------------------------------------------------------------
We can see that the performance-limiting computation here is the repeated
computation of the expression xpwr = x * xpwr. This requires a floating-
point multiplication (5 clock cycles), and the computation for one iteration
cannot begin until the one for the previous iteration has completed. The
updating of result only requires a floating-point addition (3 clock cycles)
between successive iterations.

////////////////////////////////////////////////////////////////////////////////////////

In one iteration, there are three operation executed in parallel:

* `result += PREV`, where `PREV` equals to `a[i] * xwpr`, which was calculated
  in *previous iteration*, requiring a floating-point addition (3 clock cycles).
* `a[i] * xpwr`, whose value will be used in *next iteration*, requiring a
  floating-point multiplication (5 clock cycles).
* `xpwr * x`, whose value will be used in *next iteration*, requiring a
  floating-point multiplication (5 clock cycles).

As you can see, there are no data dependencies among the three operation, so
for a superscalar out-of-order CPU, they can execute in parallel, which result in a CPE of 5.

They don't necessarily start in the same clock cycle (and in fact they can't all because the
CPU doesn't have 3 FP execution units), but one can start before another finishes, so their
latencies can overlap. They're in flight at the same time, in the pipelined FP execution units.
